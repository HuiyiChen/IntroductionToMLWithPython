{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Naive Bayes Classfiers:\n",
    "* Similar to linear models\n",
    "* In general faster in training\n",
    "* In general slightly worse performance for generalization than LinearSVC and Logistic Regression\n",
    "* Learn parameters by looking each feature individually and collect simple per-class statistics from each feature\n",
    "* Shares many of the strenghts and weakness of Linear Models, fast to train and predict, and the training procedure is easy to understand\n",
    "* They are great baseline models and are often used on very large datasets, where training even a linear model might take too long.\n",
    "\n",
    "Types:\n",
    "* GaussianNB: Continuos data,\n",
    "              Stores the average value as well as the standard deviation of each feature for each class\n",
    "              Mostly used on very high-dimensional data\n",
    "\n",
    "* BernoulliNB: Binary data, \n",
    "               It counts how often every feature of each class is not zero\n",
    "               Alpha parameter, controls the model complexity\n",
    "               Mostly used for sparse count data such as text\n",
    "\n",
    "* MultinomialNB: Count data\n",
    "                 Takes into account average value of each feature for each class\n",
    "                 Alpha parameter, controls the model complexity\n",
    "                 Mostly used for sparse count data such as text\n",
    "                 Performs better than Bernoulli particularly on datasetswith relatively large number of nonzaero features (i.e. large documents)\n",
    "\n",
    "The coef parameter does not have the same meaning than Linear algorithms, that is the w\n",
    "\n",
    "The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This results in a “smoothing” of the statistics. A large alpha means more smoothing, resulting in less complex models. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. However, tuning it usually improves accuracy somewhat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information from: http://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/\n",
    "\n",
    "Bayes theorem named after Rev. Thomas Bayes. It works on conditional probability.\n",
    "Conditional probability is the probability that something will happen, given that something else has already occurred.\n",
    "Using the conditional probability, we can calculate the probability of an event using its prior knowledge.\n",
    "\n",
    "Below is the formula for calculating the conditional probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKcAAAAaBAMAAAAgbFF0AAAAMFBMVEX///8AAADc3NxmZmYQEBB2dnZUVFS6urpERETu7u4iIiKYmJiIiIjMzMwyMjKqqqrBtYuPAAACkklEQVRIDaVUPWhTURT+eDFJ02eSihXXB2IVFY0d7CRUxMGhEocU7WImQbu8yU0UAh061LdKEdwyiBApBCVSMjiKFEQoOIpTl4Cog4N+5/68vPteAiY9kHfv+c453z03994POLQVg8NQ7B/shLhU3uzdjoD+Qrt64nkfKN37ApS3ekvwBhabYBW/hpuoowufvdX9Jl7jK8svPuBnB0eiPGJMs47YQ0dHhl+StosDknp7ljTHTtd/KdLZ8IkmJWbsgp0AxSykEb/mvWRF1wu5fd1pLgCqXIKdnkPNkAY4Nd+4EgI17/KNaO24xAuCrd5CngWO+c/+RAVgd5f5JD3orcAfmIy3f9+hiRhrofQKxT4qXGhOUlj3DZiBVzMFduA/Rlg6DWyn1Y4Jdjk+1Z0K1gI+IBdhhqTLkkJSYr4kuWZJAZbp7fuhSUmQCkaCN9Ese3BIX5QjXHU5sdAGcgFvzyeW1ffb1etRPtI53kNCi4gxkq7hbJr0N5Pfp0jFLXcMWFdjznhqkPtFE6wFbwWfSfq9cWdZQG5/c/UkJz/ES5smY6cq8DgZzmtHsK2PjUjq3e1Lp4/4K9ynrets9d02876MpYHx1FDqxNiGzDKkZ7CnSFX25B9FyqbcTuWEfzpkx/7b5gBFmjkoFEN1UJntOwuNdRRpBag0cZRr2MtfCbA0toaBoWSJVKXs9PxdIoUo/UyvZS+/U5mQLHOVnLA48XPjvGCimWdqcD2Q1EqWc1mTSc2hY0kzgjJM4SwhWSJVIy2xBUt6fmSiBROSFUuVjdmxGtiZembiqKsco+mJVRdKVvK/S6dN5ltSLh5L1WQMI7ITkmWlakTWFJCRrLGnPwUlS+qqzJGq6YiSVUqyXKmS8D8N8rApsjHXNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../../images/formula.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "    P(H) is the probability of hypothesis H being true. This is known as the prior probability.\n",
    "    P(E) is the probability of the evidence(regardless of the hypothesis).\n",
    "    P(E|H) is the probability of the evidence given that hypothesis is true.\n",
    "    P(H|E) is the probability of the hypothesis given that the evidence is there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature. We can use Wikipedia example for explaining the logic i.e.,\n",
    "\n",
    "    A fruit may be considered to be an apple if it is red, round, and about 4″ in diameter.  Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
